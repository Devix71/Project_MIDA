{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06d3ce60-8840-41ac-bea4-b54f56d96ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.transforms import functional as F\n",
    "from torchvision import models, datasets\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import random_split\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from torchvision.models import ResNet50_Weights\n",
    "from torchvision.models.detection import fasterrcnn_resnet50_fpn\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torchvision.models.detection import FasterRCNN_ResNet50_FPN_Weights\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import matplotlib.patches as patches\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data._utils.collate import default_collate\n",
    "\n",
    "def collate_fn(batch):\n",
    "    \"\"\"\n",
    "    Custom collation function for handling variable-sized target tensors.\n",
    "    Each item in the batch is a dictionary with keys 'image', 'boxes', 'labels'.\n",
    "    \"\"\"\n",
    "    # Extract images and targets from the batch\n",
    "    images = [item['image'] for item in batch]\n",
    "    targets = [{'boxes': item['boxes'], 'labels': item['labels']} for item in batch]\n",
    "\n",
    "    # Use the default_collate to collate images\n",
    "    images = default_collate(images)\n",
    "\n",
    "    return images, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ObjectDataset(Dataset):\n",
    "    def __init__(self, csv_file, img_dir):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_file (string): Path to the csv file with annotations.\n",
    "            img_dir (string): Directory with all the images.\n",
    "            transform (callable, optional): Optional transform to be applied on a sample.\n",
    "        \"\"\"\n",
    "\n",
    "        self.flags_frame = pd.read_csv(csv_file)\n",
    "\n",
    "        \n",
    "        self.img_dir = img_dir\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.flags_frame)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Find all rows corresponding to the same image\n",
    "        img_file_name = self.flags_frame.iloc[idx, 0]\n",
    "        img_rows = self.flags_frame[self.flags_frame['filename'] == img_file_name]\n",
    "\n",
    "        # Initialize lists for boxes and labels\n",
    "        boxes = []\n",
    "        labels = []\n",
    "\n",
    "        # Load image\n",
    "        img_name = os.path.join(self.img_dir, img_file_name)\n",
    "        image = Image.open(img_name)\n",
    "\n",
    "        # Iterate over all rows for this image\n",
    "        for _, row in img_rows.iterrows():\n",
    "            x, y, width, height = row['x'], row['y'], row['width'], row['height']\n",
    "            box = self.parse_box_data(x, y, width, height)\n",
    "            boxes.append(box)\n",
    "\n",
    "            label_data = row['object_type']\n",
    "            label = self.parse_label_data(label_data)\n",
    "            labels.append(label)\n",
    "\n",
    "        sample = {'image': image, 'boxes': boxes, 'labels': labels}\n",
    "\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "\n",
    "        return sample\n",
    "\n",
    "    @staticmethod\n",
    "    def parse_box_data(x,y, width, height):\n",
    "\n",
    "        # Convert to [xmin, ymin, xmax, ymax] format\n",
    "        xmin = x\n",
    "        ymin = y\n",
    "        xmax = x + width\n",
    "        ymax = y + height\n",
    "\n",
    "        return [xmin, ymin, xmax, ymax]\n",
    "\n",
    "    @staticmethod\n",
    "    def parse_label_data(label_data):\n",
    "\n",
    "        # Extract the object type\n",
    "        object_type = label_data\n",
    "\n",
    "        # Convert object types to numerical labels\n",
    "        label_dict = {'flag': 1, 'map': 1}  # Label mapping here\n",
    "        label = label_dict.get(object_type, 0)  # Default label 0 for unknown types\n",
    "\n",
    "        return label\n",
    "\n",
    "    @staticmethod\n",
    "    def transform(sample):\n",
    "        image, boxes, labels = sample['image'], sample['boxes'], sample['labels']\n",
    "        original_size = image.size  # Original size of the image (W, H)\n",
    "        \n",
    "        # Resize image\n",
    "        new_size = (448, 448)\n",
    "        image = image.convert('RGB')\n",
    "        image = F.resize(image, new_size)\n",
    "\n",
    "        # Convert image to tensor\n",
    "        image = F.to_tensor(image)\n",
    "\n",
    "        # Adjust bounding boxes\n",
    "        boxes = ObjectDataset.adjust_boxes(boxes, original_size, new_size)\n",
    "\n",
    "        return {'image': image, 'boxes': torch.as_tensor(boxes, dtype=torch.float32), 'labels': torch.as_tensor(labels, dtype=torch.int64)}\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def adjust_boxes(boxes, original_size, new_size):\n",
    "        #print(boxes)\n",
    "        x_scale = new_size[0] / original_size[0]\n",
    "        y_scale = new_size[1] / original_size[1]\n",
    "\n",
    "        adjusted_boxes = []\n",
    "        for box in boxes:\n",
    "            xmin, ymin, xmax, ymax = box\n",
    "            xmin = xmin * x_scale\n",
    "            xmax = xmax * x_scale\n",
    "            ymin = ymin * y_scale\n",
    "            ymax = ymax * y_scale\n",
    "            adjusted_boxes.append([xmin, ymin, xmax, ymax])\n",
    "\n",
    "        return adjusted_boxes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FlagDataset(Dataset):\n",
    "    def __init__(self, csv_file, img_dir):\n",
    "        self.data = []\n",
    "        flags_frame = pd.read_csv(csv_file)\n",
    "        flags_frame = flags_frame[(flags_frame[\"object_type\"] == \"flag\") & (flags_frame[\"language\"] != \"not applicable\")]\n",
    "        for _, row in flags_frame.iterrows():\n",
    "            img_file_name = row['filename']\n",
    "            img_name = os.path.join(img_dir, img_file_name)\n",
    "            image = Image.open(img_name)\n",
    "            x, y, width, height = row['x'], row['y'], row['width'], row['height']\n",
    "            cropped_image = self.crop_image(image, x, y, width, height)\n",
    "            label_data = row['language']\n",
    "            label = self.parse_label_data(label_data)\n",
    "            self.data.append((cropped_image, label))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        cropped_image, label = self.data[idx]\n",
    "        return self.transform(cropped_image), label\n",
    "\n",
    "    @staticmethod\n",
    "    def crop_image(image, x, y, width, height):\n",
    "\n",
    "        return image.crop((x, y, x + width, y + height))\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def parse_label_data(label_data):\n",
    "        label_dict = {'spanish': 1, 'russian':2, 'swedish':3}  # Update as needed\n",
    "        return label_dict.get(label_data, 0)\n",
    "\n",
    "    @staticmethod\n",
    "    def transform(image):\n",
    "        # Apply transformations\n",
    "        image = image.convert('RGB')\n",
    "        image = F.resize(image, (112, 112))\n",
    "        image = F.to_tensor(image)\n",
    "        return image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the flag dataset\n",
    "flag_dataset = FlagDataset(csv_file='data/Labelling/flags_complete.csv',\n",
    "                               img_dir='data/Images')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the object dataset\n",
    "object_dataset = ObjectDataset(csv_file='data/Labelling/flags_complete.csv',\n",
    "                               img_dir='data/Images')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_obj_size = len(object_dataset)\n",
    "train_obj_size = int(0.8 * total_obj_size)\n",
    "test_obj_size = val_obj_size = int(0.1 * total_obj_size)\n",
    "# Adjusting the train size to account for rounding\n",
    "train_obj_size += total_obj_size - (train_obj_size + test_obj_size + val_obj_size)\n",
    "\n",
    "# Ensure random splits add up to the total size\n",
    "assert train_obj_size + test_obj_size + val_obj_size == total_obj_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''total_flag_size = len(flag_dataset)\n",
    "train_flag_size = int(0.8 * total_flag_size)\n",
    "test_flag_size = val_flag_size = int(0.1 * total_flag_size)\n",
    "# Adjusting the train size to account for rounding\n",
    "train_flag_size += total_flag_size - (train_flag_size + test_flag_size + val_flag_size)\n",
    "\n",
    "# Random splits add up to the total size\n",
    "assert train_flag_size + test_flag_size + val_flag_size == total_flag_size'''\n",
    "\n",
    "total_flag_size = len(flag_dataset)\n",
    "train_flag_size = int(0.8 * total_flag_size)  # 80% of the dataset for training\n",
    "val_flag_size = total_flag_size - train_flag_size  # The rest for validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_flag_dataset, val_flag_dataset, test_flag_dataset = random_split(flag_dataset, [train_flag_size, val_flag_size, test_flag_size])\n",
    "\n",
    "train_flag_dataset, val_flag_dataset = random_split(flag_dataset, [train_flag_size, val_flag_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_object_dataset, val_object_dataset, test_object_dataset = random_split(object_dataset, [train_obj_size, val_obj_size, test_obj_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16  # batch size\n",
    "\n",
    "train_flag_loader = DataLoader(train_flag_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_flag_loader = DataLoader(val_flag_dataset, batch_size=batch_size, shuffle=False)\n",
    "#test_flag_loader = DataLoader(test_flag_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8  # batch size\n",
    "\n",
    "train_loader = DataLoader(train_object_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "val_loader = DataLoader(val_object_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "test_loader = DataLoader(test_object_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def show_image_with_label(dataset, idx, label_map=None):\n",
    "    image, label = dataset[idx]\n",
    "\n",
    "    # Convert tensor image to numpy array\n",
    "    if isinstance(image, torch.Tensor):\n",
    "        np_image = image.cpu().numpy().transpose(1, 2, 0)\n",
    "    else:\n",
    "        np_image = np.array(image)\n",
    "\n",
    "    # Normalize and display the image\n",
    "    np_image = np_image - np_image.min()\n",
    "    np_image = np_image / np_image.max()\n",
    "\n",
    "    # Create a matplotlib figure\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.imshow(np_image)\n",
    "\n",
    "    # Display the label\n",
    "    label_text = f'{label}' if label_map is None else label_map.get(label, 'Unknown')\n",
    "    ax.set_title(label_text)\n",
    "    ax.axis('off')  # Turn off axis\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "# Label map\n",
    "label_map = { 1: 'Spanish', 2: \"Russian\", 3: \"Swedish\"}\n",
    "\n",
    "# Preview first few images in the dataset\n",
    "for i in range(5):\n",
    "    show_image_with_label(flag_dataset, 500+i, label_map)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_image_with_boxes(dataset, idx, label_map=None):\n",
    "    sample = dataset[idx]\n",
    "    image, boxes, labels = sample['image'], sample['boxes'], sample['labels']\n",
    "\n",
    "    if not isinstance(image, torch.Tensor):\n",
    "        image = F.to_tensor(image)\n",
    "    np_image = image.cpu().numpy().transpose(1, 2, 0)\n",
    "    np_image = np_image - np_image.min()\n",
    "    np_image = np_image / np_image.max()\n",
    "\n",
    "    fig, ax = plt.subplots(1)\n",
    "    ax.imshow(np_image)\n",
    "\n",
    "\n",
    "\n",
    "    for box, label in zip(boxes, labels):\n",
    "        xmin, ymin, xmax, ymax = box\n",
    "\n",
    "        rect = patches.Rectangle((xmin, ymin), xmax - xmin, ymax - ymin, linewidth=1, edgecolor='r', facecolor='none')\n",
    "        ax.add_patch(rect)\n",
    "\n",
    "        label_value = label.item()\n",
    "        label_text = f'{label_value}' if label_map is None else label_map.get(label_value, 'Unknown')\n",
    "        ax.text(xmin, ymin, label_text, bbox=dict(facecolor='white', alpha=0.5))\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "# Label map\n",
    "label_map = {1: 'Flag'}\n",
    "\n",
    "# Preview first few images in the dataset\n",
    "for i in range(5):\n",
    "    show_image_with_boxes(object_dataset, i, label_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "911abc42-428c-491c-adc2-7078ea52b6e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Object Detection Model\n",
    "object_detection_model = fasterrcnn_resnet50_fpn(weights=FasterRCNN_ResNet50_FPN_Weights.DEFAULT)\n",
    "\n",
    "object_detection_model.to(device)\n",
    "# Freeze certain layers for fine-tuning\n",
    "for layer in object_detection_model.parameters():\n",
    "    layer.requires_grad = False\n",
    "\n",
    "# Modify the model for the desired number of classes (1 + background)\n",
    "in_features = object_detection_model.roi_heads.box_predictor.cls_score.in_features\n",
    "object_detection_model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image Classification Model\n",
    "#device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "image_classification_model = torchvision.models.resnet50(weights=ResNet50_Weights.DEFAULT)\n",
    "image_classification_model.to(device)\n",
    "\n",
    "# Freeze all layers except the last one\n",
    "for param in image_classification_model.parameters():\n",
    "    param.requires_grad = False\n",
    "image_classification_model.fc.requires_grad = True\n",
    "\n",
    "# Replace the pre-trained head with a new one\n",
    "num_classes = 4  # Spanish, Russian and Swedish implemented in the dataset so far\n",
    "image_classification_model.fc = nn.Linear(image_classification_model.fc.in_features, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "071a0ce5-646e-4c57-bdf0-585093ecda5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "params = [p for p in object_detection_model.parameters() if p.requires_grad]\n",
    "object_detection_model = object_detection_model.to(device)\n",
    "\n",
    "\n",
    "optimizer = torch.optim.SGD(params, lr=0.005, momentum=0.9)\n",
    "scheduler = StepLR(optimizer, step_size=3, gamma=0.1)\n",
    "num_epochs = 10\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    object_detection_model.train()\n",
    "    train_loss = 0.0\n",
    "\n",
    "    # Wrap train_loader with tqdm for a progress bar\n",
    "    progress_bar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{num_epochs} [Training]')\n",
    "    for images, targets in progress_bar:\n",
    "        \n",
    "        images = list(img.to(device) for img in images)\n",
    "\n",
    "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "        # Check device for each target tensor\n",
    "        '''\n",
    "        for target in targets:\n",
    "            for key, value in target.items():\n",
    "                if isinstance(value, torch.Tensor):\n",
    "                    print(f\"Target tensor '{key}' device:\", value.device)\n",
    "        '''\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss_dict = object_detection_model(images, targets)\n",
    "        losses = sum(loss for loss in loss_dict.values())\n",
    "        losses.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += losses.item()\n",
    "        # Update progress bar with current loss\n",
    "        progress_bar.set_postfix({'train_loss': losses.item()})\n",
    "\n",
    "    # Validation phase\n",
    "    object_detection_model.eval()  # Model is in eval mode for correct behavior\n",
    "    val_loss = 0.0\n",
    "    progress_bar = tqdm(val_loader, desc=f'Epoch {epoch+1}/{num_epochs} [Validation]')\n",
    "    with torch.no_grad():\n",
    "        for images, targets in progress_bar:\n",
    "            images = list(img.to(device) for img in images)\n",
    "            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "            # Temporarily set the model to training mode to compute the loss\n",
    "            object_detection_model.train()\n",
    "            loss_dict = object_detection_model(images, targets)\n",
    "            object_detection_model.eval()  # Set it back to eval mode\n",
    "\n",
    "            losses = sum(loss for loss in loss_dict.values())\n",
    "            val_loss += losses.item()\n",
    "            # Update progress bar with current validation loss\n",
    "            progress_bar.set_postfix({'val_loss': losses.item()})\n",
    "\n",
    "    # Learning rate scheduler step\n",
    "    scheduler.step()\n",
    "\n",
    "    # Early stopping check\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        # Save the best model\n",
    "        torch.save(object_detection_model.state_dict(), 'best_object_detection_model.pth')\n",
    "\n",
    "    print(f'Epoch {epoch}, Train Loss: {train_loss / len(train_loader)}, Val Loss: {val_loss / len(val_loader)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c48c69ff-b742-49bc-b4c3-3de7f35598fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "#device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "image_classification_model = image_classification_model.to(device)\n",
    "\n",
    "# Define the optimizer and scheduler\n",
    "optimizer = torch.optim.SGD(image_classification_model.parameters(), lr=0.001, momentum=0.9)\n",
    "scheduler = StepLR(optimizer, step_size=3, gamma=0.1)\n",
    "criterion = nn.CrossEntropyLoss()  # Using CrossEntropyLoss\n",
    "\n",
    "num_epochs = 10\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Training phase\n",
    "    image_classification_model.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    progress_bar = tqdm(train_flag_loader, desc=f'Epoch {epoch+1}/{num_epochs} [Training]')\n",
    "    for inputs, labels in progress_bar:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = image_classification_model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        progress_bar.set_postfix({'train_loss': loss.item()})\n",
    "\n",
    "    # Validation phase\n",
    "    image_classification_model.eval()\n",
    "    val_loss = 0.0\n",
    "    progress_bar = tqdm(val_flag_loader, desc=f'Epoch {epoch+1}/{num_epochs} [Validation]')\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in progress_bar:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = image_classification_model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "            progress_bar.set_postfix({'val_loss': loss.item()})\n",
    "\n",
    "    scheduler.step()\n",
    "\n",
    "    # Early stopping check\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save(image_classification_model.state_dict(), 'best_image_classification_model.pth')\n",
    "\n",
    "    print(f'Epoch {epoch}, Train Loss: {running_loss / len(train_flag_loader)}, Val Loss: {val_loss / len(val_flag_loader)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b137f42-990d-4716-ba5a-d8190441370c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load fine-tuned object detection model\n",
    "object_detection_model.load_state_dict(torch.load('best_object_detection_model.pth', map_location=torch.device('cpu')))\n",
    "object_detection_model.to('cpu')\n",
    "object_detection_model.eval()\n",
    "\n",
    "# Load fine-tuned image classification model\n",
    "image_classification_model.load_state_dict(torch.load('best_image_classification_model.pth', map_location=torch.device('cpu')))\n",
    "image_classification_model.to('cpu')\n",
    "image_classification_model.eval()\n",
    "\n",
    "\n",
    "# Define transforms\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((448, 448)),  # Resize the image\n",
    "    transforms.ToTensor(),\n",
    "    \n",
    "])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24b12d3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the image and apply transforms\n",
    "image_path = \"testerpic.jpg\"  # Path to the image\n",
    "image = Image.open(image_path)\n",
    "if image.mode != 'RGB':\n",
    "    image = image.convert('RGB')\n",
    "image = transform(image)  # This will be a 3D tensor [C, H, W]\n",
    "image = image.to(device)\n",
    "\n",
    "# Detect objects in the image\n",
    "object_detection_results = object_detection_model([image])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7ad21f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def evaluate_image_classification_model(model, test_loader):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_true = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_true.extend(labels.cpu().numpy())\n",
    "\n",
    "    # Calculate and print the metrics\n",
    "    print(classification_report(all_true, all_preds))\n",
    "    print(\"Accuracy:\", accuracy_score(all_true, all_preds))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d7571216",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.38      0.16      0.22        76\n",
      "           1       0.13      0.15      0.14        34\n",
      "           2       0.14      0.26      0.19        38\n",
      "           3       0.21      0.25      0.23        40\n",
      "\n",
      "    accuracy                           0.20       188\n",
      "   macro avg       0.21      0.20      0.19       188\n",
      "weighted avg       0.25      0.20      0.20       188\n",
      "\n",
      "Accuracy: 0.19680851063829788\n"
     ]
    }
   ],
   "source": [
    "evaluate_image_classification_model(image_classification_model, val_flag_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ddd8491-6c03-4005-a6c4-9acddab42bfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize lists for storing patches\n",
    "flag_image_patches = []\n",
    "map_image_patches = []\n",
    "\n",
    "# Extract detection results\n",
    "detected_boxes = object_detection_results[0]['boxes']\n",
    "detected_labels = object_detection_results[0]['labels']\n",
    "detected_scores = object_detection_results[0]['scores']\n",
    "\n",
    "# Threshold for detection confidence\n",
    "detection_threshold = 0.8\n",
    "\n",
    "# Iterate over each detection\n",
    "for i, box in enumerate(detected_boxes):\n",
    "    if detected_scores[i] > detection_threshold:\n",
    "        # Extract bounding box coordinates\n",
    "        xmin, ymin, xmax, ymax = map(int, box.tolist())\n",
    "\n",
    "        # Extract image patch based on the bounding box\n",
    "        image_patch = image[:, ymin:ymax, xmin:xmax].unsqueeze(0)\n",
    "\n",
    "        # Check label and append to corresponding list\n",
    "        if detected_labels[i] == 1:  # Label 1 corresponds to 'flag'\n",
    "            flag_image_patches.append(image_patch)\n",
    "        elif detected_labels[i] == 2:  # Label 2 corresponds to 'map'\n",
    "            flag_image_patches.append(image_patch)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7376f4b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Convert the tensor image back to PIL for visualization\n",
    "np_image = image.cpu().squeeze(0).numpy()\n",
    "np_image = np.transpose(np_image, (1, 2, 0))  # Change channel order for matplotlib\n",
    "np_image = np_image - np_image.min()  # Normalize for display\n",
    "np_image = np_image / np_image.max()\n",
    "\n",
    "# Create a matplotlib figure\n",
    "fig, ax = plt.subplots(1, figsize=(12, 8))\n",
    "ax.imshow(np_image)\n",
    "\n",
    "# Add bounding boxes to the image\n",
    "for i, box in enumerate(detected_boxes):\n",
    "    if detected_scores[i] > detection_threshold:\n",
    "        xmin, ymin, xmax, ymax = map(int, box.tolist())\n",
    "        rect = patches.Rectangle((xmin, ymin), xmax - xmin, ymax - ymin, linewidth=2, edgecolor='red', facecolor='none')\n",
    "        ax.add_patch(rect)\n",
    "\n",
    "        # Add label text\n",
    "        label = detected_labels[i].item()\n",
    "        label_text = 'Flag' if label == 1 else 'Flag' if label == 2 else 'Unknown'\n",
    "        ax.text(xmin, ymax, label_text, color='white', fontsize=12, bbox=dict(facecolor='red', alpha=0.5))\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c79dd663-7504-417a-a707-43af8baedc95",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Classify each flag image patch and get probabilities for all predictions\n",
    "flag_probabilities = []\n",
    "for flag_image_patch in flag_image_patches:\n",
    "    flag_image_patch = flag_image_patch.to(device)\n",
    "\n",
    "    # Get model predictions (raw scores)\n",
    "    with torch.no_grad():\n",
    "        flag_outputs = image_classification_model(flag_image_patch)\n",
    "\n",
    "    # Apply softmax to convert raw scores to probabilities\n",
    "    probabilities = F.softmax(flag_outputs, dim=1)\n",
    "\n",
    "    # Convert probabilities to numpy for easy manipulation\n",
    "    probabilities_np = probabilities.cpu().numpy()\n",
    "\n",
    "    # Append the probabilities to the flag_probabilities list\n",
    "    flag_probabilities.append(probabilities_np)\n",
    "\n",
    "    # Print the probabilities for each class\n",
    "    print(probabilities_np)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "655feee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_detection_and_classification(image, detected_boxes, detected_scores, detected_labels, classification_probabilities, detection_threshold=0.7, label_map=None):\n",
    "    # Convert tensor image to numpy array for display\n",
    "    np_image = image.cpu().squeeze(0).numpy().transpose(1, 2, 0)\n",
    "    np_image = np_image - np_image.min()\n",
    "    np_image = np_image / np_image.max()\n",
    "\n",
    "    fig, ax = plt.subplots(1, figsize=(12, 8))\n",
    "    ax.imshow(np_image)\n",
    "\n",
    "    for i, box in enumerate(detected_boxes):\n",
    "        if detected_scores[i] > detection_threshold:\n",
    "            xmin, ymin, xmax, ymax = map(int, box.tolist())\n",
    "            rect = patches.Rectangle((xmin, ymin), xmax - xmin, ymax - ymin, linewidth=2, edgecolor='red', facecolor='none')\n",
    "            ax.add_patch(rect)\n",
    "\n",
    "            detection_label = detected_labels[i].item()\n",
    "            detection_label_text = 'Flag' if detection_label == 1 else 'Flag' if detection_label == 2 else 'Unknown'\n",
    "            \n",
    "            # Get classification label with highest probability\n",
    "            classification_prob = classification_probabilities[i][0]  # One flag per image patch\n",
    "            classification_prob[0] = classification_prob[0]/2.5 # Applying weight\n",
    "            print(classification_prob)\n",
    "            classification_label = np.argmax(classification_prob)\n",
    "            print(classification_label)\n",
    "            classification_label_text = label_map.get(classification_label, 'Unknown')\n",
    "\n",
    "            combined_label_text = f'{detection_label_text} | {classification_label_text} ({classification_prob[classification_label]:.2f})'\n",
    "            ax.text(xmin, ymax, combined_label_text, color='white', fontsize=12, bbox=dict(facecolor='red', alpha=0.5))\n",
    "\n",
    "    plt.axis('off')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "661f4139",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map = { 1: 'Spanish', 2: \"Russian\", 3: \"Swedish\"}\n",
    "\n",
    "visualize_detection_and_classification(image, detected_boxes, detected_scores, detected_labels, flag_probabilities, label_map=label_map)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
